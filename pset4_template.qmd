---
title: "Problem Set 4: Spatial"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 

## Style Points (10 pts)

## Submission Steps (10 pts)

## Download and explore the Provider of Services (POS) file (10 pts)

(Note: Used ChatGPT to troubleshoot why pd.read_csv wasn't working- learned to specify a different encoding other than utf-8)


1. Which variables do we need to complete the exercise:
- Short-term hospitals
  -Certification Date (CRTFCTN_DT)
  -Change of ownership date (CHOW_DT)
  -Provider code 1 (PRVDR_CTGRY_CD)
  -subtype 1 (PRVDR_CTGRY_SBTYP_CD)
  -CMS Certification number (PRVDR_NUM)
  -Termination Code (PGM_TRMNTN_CD)
  -Termination Date (TRMNTN_EXPRTN_DT)
  -Facility Name (FAC_NAME)
  -Facility Zip (ZIP_CD)

2. 
    a. Import the pos2016.csv file and subset it:


```{python}
import pandas as pd
df = pd.read_csv('pos_2016.csv', encoding='Windows-1252', low_memory=False)

df.columns = ["prvdr_subtype_code", "prvdr_category_code", "change_of_owner_date", "certification_date",
              "facility_name", "prvdr_num", "zip_code", "termination_code", "termination_date"]

# The date fields are numbers, so we need to change the numbers in the date fields to strings and then date objects. Attribution: Used ChatGPT to assist with regex issues
df['change_of_owner_date'] = pd.to_datetime(df['change_of_owner_date'].astype(
    str).str.replace('.0', '', regex=False), format='%Y%m%d')

df['certification_date'] = pd.to_datetime(df['certification_date'].astype(
    str).str.replace('.0', '', regex=False), format='%Y%m%d')

df['termination_date'] = pd.to_datetime(df['termination_date'].astype(
    str).str.replace('.0', '', regex=False), format='%Y%m%d')

# Convert zip code and termination codes to strings
df['termination_code'] = df['termination_code'].astype(str)
df['zip_code'] = df['zip_code'].astype(str)


df = df[(df["prvdr_category_code"] == 1) & (df["prvdr_subtype_code"] == 1)]

```

  b. After subsetting to only short-term hospitals with these codes, we find 7245 hospitals in our data set. The provided KFF article cites a number of "nearly 5000".

This discrepancy could be due to potential duplicates in our data (if a hospital was acquired or merged into a newer hospital, it may be counted twice). Additionally, our data may include hospitals that are short-term hospitals, but not specifically acute-care hospitals providing emergency care/surgery and other interventions, which the KFF article might exclude.

3. Repeat the previous steps with 2017Q4, 2018Q4, and 2019Q4 and then append them together: 

```{python}

def read_clean_filter(pathname):
  """Reads the raw csv files from CMS, renames the columns, and filters to only short term hospitals.
  Also updates the date columns to pd.datetime format
  """
  df = pd.read_csv(pathname, encoding='Windows-1252', low_memory=False)

  df.columns = ["prvdr_subtype_code", "prvdr_category_code", "change_of_owner_date", "certification_date", "facility_name", "prvdr_num", "zip_code", "termination_code", "termination_date"]
  
  df['change_of_owner_date'] = pd.to_datetime(df['change_of_owner_date'].astype(str).str.replace('.0', '', regex=False), format='%Y%m%d')

  df['certification_date'] = pd.to_datetime(df['certification_date'].astype(str).str.replace('.0', '', regex=False), format='%Y%m%d')

  df['termination_date'] = pd.to_datetime(df['termination_date'].astype(str).str.replace('.0', '', regex=False), format='%Y%m%d')

  df['termination_code'] = df['termination_code'].astype(str)
  df['zip_code'] = df['zip_code'].astype(str)

  df = df[(df["prvdr_category_code"] == 1) & (df["prvdr_subtype_code"] == 1)]

  return df

df_2017 = read_clean_filter("pos_2017.csv")
df_2018 = read_clean_filter("pos_2018.csv")
df_2019 = read_clean_filter("pos_2019.csv")

#Merge all dataframes from 2016-2019 together
#Use this for future pset problems

df_all = pd.concat([df, df_2017, df_2018, df_2019], ignore_index=True)

```

Now we plot the # of observations in the dataset by year of certification date:

```{python}

import altair as alt

df_all['year'] = df_all['certification_date'].dt.year
summary = df_all.groupby('year').size().reset_index(name='count')

observations_by_year = alt.Chart(summary).mark_bar().encode(
  x = alt.X('year:O', title="Year"),
  y = alt.Y('count', title="Number of Observations")
)

observations_by_year

```

4. 
a. Now we repeat the previous step but this time plot unique hospitals by year instead of just observations:

```{python}

unique_hospitals = df_all.drop_duplicates(subset=['prvdr_num', 'year'])
unique_summary = unique_hospitals.groupby('year').size().reset_index(name='count')

unique_hospitals_by_year = alt.Chart(unique_summary).mark_bar().encode(
  x = alt.X('year:O', title="Year"),
  y = alt.Y('count', title="Number of Unique Hospitals")
)

unique_hospitals_by_year

```


b. Not only are the plots different, we can see two things: 

1. The maximum number of hospitals is much smaller than the maximum number of observations, implying there are a lot of repeat rows in the data for the same facilities.
2. The graph of unique hospitals is much more skewed to the right than simply graphing observations, implying that either many new hospitals were certified from 2014 onwards, or that many old hospitals were merged/rebranded into new hospitals from 2014 onwards. Based on what we've read in the KFF article, it's highly likely that recent corporate/economic pressures caused hospitals to merge/rebrand leading to a rush of new certifications, rather than an actual increase in the amount of hospital capacity.


## Identify hospital closures in POS file (15 pts) (*)

1. Create a list of hospitals active in 2016, closed by 2019 (facility name, zip, year of closure). How many are there?


```{python}
#Create a list of all hospitals active in 2016:
df_2016_active = df[df["termination_code"] == "0"]

#Now go back to original dataset and find zip codes where the number of active hospitals does not decrease in the year after the suspected closure.

#By definition, we don't have 2020 data for 2019 closures, and if we are looking at all 2016 active hospitals, 2017 is irrelevant.
#So we are looking only at hospitals that closed in 2017 and 2018.

closed_in_1718 = df_all[(df_all["termination_date"] >= "2017-01-01") & (df_all["termination_date"] <= "2018-12-31")]

#Now remove duplicates:
closed_in_1718 = closed_in_1718.drop_duplicates(subset='prvdr_num')

#These are the hospitals in our dataset that closed in 2017-2018.
print(f"There are {len(closed_in_1718)} hospitals that closed during this period.")

```

2. Sort this list of hospitals by name and report the names and year of
suspected closure for the first 10 rows.

```{python}
closed_in_1718.sort_values(by='facility_name', ascending=True, inplace=True)

closed_in_1718.head(10)

```

3. However, not all suspected hospital closures are true closures. For example,
in the case of a merger, a CMS certification number will be appear to be “terminated,” but then the hospital re-appear under a similar name/address with a new CMS certification number in the next year. As a first pass to address this, remove any suspected hospital closures that are in zip codes where the number of active hospitals does not decrease in the year after the suspected closure.


```{python}
#Now for each closure, we need to check its zip code and see if it was a zip code where the number of active hospitals did not decrease the year after its closure.
#Each df_201x dataset is a snapshot of the state of hospitals in Q4-2017 or Q4-2018.

#How about this? Take the df_2017 data set only, and do a summary that counds the number of active hospitals by zip code. Then do the same for df_2018 and df_2019. Now we have three tables showing the breakdown of each zipcodes' active hospitals for each year.

#Attribution: Originally tried this with a for loop, but ChatGPT suggested using a dictionary to store the summary tables instead

active_hospital_summaries = {}

# Iterate through each DataFrame and generate the summary table
for year, df_x in zip(['2017', '2018', '2019'], [df_2017, df_2018, df_2019]):

    df_x['is_active'] = df_x['termination_code'] == "0"
    active_hospitals = df_x.groupby('zip_code')['is_active'].sum().reset_index()
    
    # Store summary table in the dictionary 
    active_hospital_summaries[f'active_hospitals_{year}'] = active_hospitals

#for readability
ah_2017 = active_hospital_summaries['active_hospitals_2017']
ah_2018 = active_hospital_summaries['active_hospitals_2018']
ah_2019 = active_hospital_summaries['active_hospitals_2019']


#Now we need to use these tables to create a way for us to check: when we provide our function a hospital closure, how can our function check to see if it was in a zip code where the number of active hospitals did not decrease the year after its closure?
#We can do this by taking our summary tables and merging them:

ah_2017 = ah_2017.rename(columns={'is_active': 'active_2017'})
ah_2018 = ah_2018.rename(columns={'is_active': 'active_2018'})

comp_2017_2018 = pd.merge(ah_2017, ah_2018, on="zip_code", how="outer")

#This handles any zip codes that were not present in 2017 or 2018 and now have a NA as part of the merge
comp_2017_2018.fillna(0, inplace=True)

#Now create a col that puts a boolean value if a zip code's active hospitals decreased from 2017-2018
comp_2017_2018['did_decrease'] = (comp_2017_2018['active_2017'] > comp_2017_2018['active_2018']).astype(int)

#And repeat for 2018-2019
ah_2019 = ah_2019.rename(columns={'is_active': 'active_2017'})
comp_2018_2019 = pd.merge(ah_2018, ah_2019, on="zip_code", how="outer")
comp_2018_2019.fillna(0, inplace=True)
comp_2018_2019['did_decrease'] = (comp_2018_2019['active_2017'] > comp_2018_2019['active_2018']).astype(int)

#Now we have our two comparison tables, so let's write a function that goes through closed_in_1718 that does the following:


def check_zip(closed_df, comp_2017_2018, comp_2018_2019):
  """
  #create a placeholder list for decreasing zipcodes (see below)
  #take the zipcode of the current row and check the termination_date's year
  #depending on whether the termination_date's year is 2017 or 2018, refer to the comp_2017_2018 or the comp_2018_2019 table
  #take the zipcode of the current row and check whether that zipcode has a 0 or 1 in the "did_decrease" column
  #if that zipcode has a 1 in that column, add it to the decreasing zipcodes list
  #the function should return a list of all zipcodes that had decreasing hospitals
  """

  decreasing_zipcodes = []

  zip_codes = closed_df['zip_code']
  termination_years = closed_df['termination_date'].dt.year

  for zip_code, year in zip(zip_codes, termination_years):
      # Select the appropriate comparison table based on the termination year
      if year == 2017:
          comp_table = comp_2017_2018
      elif year == 2018:
          comp_table = comp_2018_2019
      else:
          # Skip if the termination year is not 2017 or 2018
          continue

      # Check if the zip code exists in the comparison table and if it shows a decrease
      if zip_code in comp_table['zip_code'].values:
          did_decrease = comp_table.loc[comp_table['zip_code'] == zip_code, 'did_decrease'].values[0]

          # If the zip code had a decrease in active hospitals, add it to the list
          if did_decrease == 1:
              decreasing_zipcodes.append(zip_code)

  return decreasing_zipcodes


decreasing_zipcodes = check_zip(closed_in_1718, comp_2017_2018, comp_2018_2019)

print(decreasing_zipcodes)


```

  a. Now we have a list of decreasing zipcodes, these are likely mergers/acquisitions:

  ```{python}
  print(f"There are {len(decreasing_zipcodes)} hospitals that fit this definition.")

  ```

  b. After correcting for this, how many hospitals do you have left?
  
  94 - 12 hospitals = 82 hospitals

  c. Sort this list of corrected hospital closures by name and report the first 10 rows.

```{python}
# Attribution: ChatGPT- Using ~ is the "not" operator (originally tried to do this with an != sign)
corrected_closures = closed_in_1718[~closed_in_1718['zip_code'].isin(decreasing_zipcodes)]

corrected_closures.sort_values(by='facility_name', ascending=True, inplace=True)

corrected_closures.head(10)

```

## Download Census zip code shapefile (10 pt) 


1. 
    a. The five file types are:
      1. .shp - The file that contains the actual geoms (points, lines, polygons) that will be displayed
      2. .dbf - Stores attribute data for each shape in the .shp file, like population, income, temperature, etc. 
      3. .shx - An index file that points to specific shapes in the .shp file for easier navigation
      4. .prj - A projection file that stores the coordinate information for latitude/longitude 
      5. .xml - Metadata file that is mostly used for documentation/readme purposes, including data sources, attributes, etc.
    b. The .shp file is the largest at 837.5 MB, while the other files are:
      .dbf - 6.4 MB
      .shx - 0.26 MB
      .xml - 16 KB
      .prj - 0.16 KB
2. To open the shapefile, we will install and use geopandas:

```{python}
import geopandas as gpd
zip_codes = gpd.read_file("/Users/charleshuang/Downloads/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp")

#Texas zip codes always start with 75-79 and ZCTA5 is the column with the zip codes
#Used ChatGPT to find a function that can identify strings by the first few characters
tx_zip_codes = zip_codes[zip_codes["ZCTA5"].str.startswith(('75', '76', '77', '78', '79'))]

print(tx_zip_codes.head())
```

Now we calculate the # of hospitals per zip code in 2016 based on the previous step 
(note: we are using the entire dataset of hospitals short term and active in 2016- this is the initial df file from Q4 2016)

```{python}
#Note: This counts only short term, active hospitals in Texas

texas_df_2016 = df[df['zip_code'].astype(str).str.startswith(('75', '76', '77', '78', '79'))]


texas_summary_2016 =  texas_df_2016.groupby('zip_code').size().reset_index()

#Remove trailing decimal points from zip code and convert it to string. Attribution: ChatGPT suggested using zfill() to strip the decimal point
texas_summary_2016['zip_code'] = texas_summary_2016['zip_code'].astype(str).str.split('.').str[0].str.zfill(5)

texas_summary_2016.columns = ['zip_code', 'hospital_count']
print(texas_summary_2016.sort_values(by="hospital_count", ascending=False))

```


Now we will create a choropleth of the hospitals in Texas by zip code:

```{python}

import shapely
from shapely import Polygon, Point
import geopandas as gpd
import matplotlib.pyplot as plt

#We need to merge our previous hospital data with the shapefile zip_codes data:


tx_zip_hospitals = tx_zip_codes.merge(texas_summary_2016, left_on="ZCTA5", right_on="zip_code", how="left")

# Replace NaN hospital counts with 0
tx_zip_hospitals['hospital_count'] = tx_zip_hospitals['hospital_count'].fillna(0).astype(int)

#Attribution: ChatGPT suggested defining custom bins to make it easier to differentiate regions


tx_zip_hospitals.plot(
    column='hospital_count',
    cmap='PuBu',
    legend=True
)

plt.title("Number of Hospitals by Zip Code in Texas")
plt.axis('off')

plt.show()


```



## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
    c.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. Using the corrected hospital closures dataset from the first section, create a list of directly affected zip codes in Texas – that is, those with at least one closure in 2016-2019. Display a table of the number of zip codes vs. the number of closures they experienced.


Returning to the corrected_closures data from the last section, we have 82 hospitals whose zips we will pull:
```{python}

zip_closures = corrected_closures.groupby("zip_code").size().reset_index()

zip_closures['zip_code'] = zip_closures['zip_code'].astype(str).str.split('.').str[0].str.zfill(5)

zip_closures.columns = ["zip_code", "closures"]
zip_closures.sort_values("closures", ascending=False).head(10)



```


2. Plot a choropleth of which Texas zip codes were directly affected by a closure in 2016-2019 – there was at least one closure within the zip code. How many directly affected zip codes are there in Texas?


To do this, we will again merge the tx_zip_codes data with our table from above:

```{python}


tx_hospital_closures = tx_zip_codes.merge(zip_closures, left_on="ZCTA5", right_on="zip_code", how="left")

#This line replaces NAs from the merge with zeroes- because zip codes not on our zip_closures list have 0 closures
tx_hospital_closures['closures'] = tx_hospital_closures['closures'].fillna(0).astype(int)

tx_hospital_closures.plot(
    column='closures',
    cmap='Reds',
)
plt.title("Zip Codes With Hospital Closures in Texas, 2016-2019")
plt.axis('off')

plt.show()

```

```{python}
print(f"There were {len(zip_closures)} directly affected zip codes in Texas during this period.")

```

```{python}
#test code
tx_hospital_closures.sort_values("closures", ascending=False).head()

```

3. Then identify all the indirectly affected zip codes: Texas zip codes within a 10-mile radius of the directly affected zip codes. To do so, first create a GeoDataFrame of the directly affected zip codes. Then create a 10-mile buffer around them. Then, do a spatial join with the overall Texas zip code shapefile. How many indirectly affected zip codes are there in Texas?

``` {python}
#Buffer is measured in meters, so 10 meters is approx. 16,100 meters
#Our directly affected zipcodes geodataframe is tx_hospital_closures

#First turn zip_closures into a geodataframe, not a dataframe

zip_closures_geo = tx_zip_codes.merge(zip_closures, left_on="ZCTA5", right_on="zip_code", how="inner",  suffixes=("_tx", "_closure"))
zip_closures_geo = zip_closures_geo.set_geometry('geometry') 

tx_zip_codes = tx_zip_codes.to_crs(epsg=3857)
zip_closures_geo = zip_closures_geo.to_crs(epsg=3857)


#Need to store the buffer result in a new GeoDataFrame
zip_closures_geo['buffered_geometry'] = zip_closures_geo.geometry.buffer(16093.4)
buffer_zone = zip_closures_geo.set_geometry('buffered_geometry')

#Spatial join
spatial_join = gpd.sjoin(tx_zip_codes, buffer_zone, how="inner", predicate="intersects", lsuffix="_left", rsuffix="_right")

```

```{python}

num_indirectly_affected = spatial_join['ZCTA5__left'].nunique()
print(f"Number of indirectly affected zip codes: {num_indirectly_affected}")

```

```{python}
#Test code
print(spatial_join.columns)


```


4. Make a choropleth plot of the Texas zip codes with a different color for each of the 3 categories: directly affected by a closure, within 10 miles of closure but
not directly affected, or not affected.

To do this, we need to go back to tx_zip_codes and classify each zip code in one of the three categories:

```{python}

#Create a new status column and set all to Not Affected at first, then edit specific zips to be affected later
tx_zip_codes['status'] = "Not Affected"

#Directly affected
directly_affected_zips = zip_closures_geo['ZCTA5'].unique()
tx_zip_codes.loc[tx_zip_codes['ZCTA5'].isin(directly_affected_zips), 'status'] = 'Directly Affected'
print("Directly affected zips:", directly_affected_zips) 

#Indirectly affected (but not directly affected)
#This will require a spatial join
# Perform the spatial join and rename the column immediately after for consistency
spatial_join_df = gpd.sjoin(tx_zip_codes, buffer_zone, how="inner", predicate="intersects")
spatial_join_df = spatial_join_df.rename(columns={'ZCTA5_left': 'ZCTA5'})

indirectly_affected_zips = spatial_join_df['ZCTA5'].unique()

# Filter out directly affected zip codes and update 'status'
indirectly_affected_filtered = [z for z in indirectly_affected_zips if z not in directly_affected_zips]
tx_zip_codes.loc[tx_zip_codes['ZCTA5'].isin(indirectly_affected_filtered), 'status'] = 'Indirectly Affected'

#Plot the plots with tx_zip_code and status

fig, ax = plt.subplots()
tx_zip_codes.plot(column='status', cmap = 'Set1', legend=True, ax=ax)

legend = ax.get_legend()
if legend:  
    legend.set_title("Zip Code Status")

plt.title("Texas Zip Codes by Affected Status")
plt.axis("off")
plt.show()
```

## Reflecting on the exercise (10 pts) 
